{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 188us/step - loss: 0.7062 - acc: 0.5090\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 0.6953 - acc: 0.5080\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.6891 - acc: 0.5390\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.6845 - acc: 0.5460\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.6803 - acc: 0.5490\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 0.6757 - acc: 0.5670\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 0.6712 - acc: 0.5810\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.6671 - acc: 0.5960\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 46us/step - loss: 0.6618 - acc: 0.6110\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 0.6535 - acc: 0.6110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x165846af860>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# For a single-input model with 2 classes (binary classification):\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2\n",
    "\n",
    "print('Loading data...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word', 'another', 'the', 'trump', 'rallies', 'packed', 'the', 'venues', 'though', 'thousands', 'of', 'people', 'who', 'all', 'got', 'to', 'see', 'the', 'man', 'himself', 'in', 'front', 'of', 'them', 'super', 'impressive', 'i', 'went', 'to', 'one', 'of', 'hillary’s', 'rallies', 'to', 'see', 'the', 'debacle', 'for', 'myself', 'she', 'didn’t', 'answer', 'a', 'damn', 'question', 'not', 'one', 'a', 'simple', '30m', 'speech', 'to', 'approximately', '300', 'people', 'in', 'a', 'small', 'gym', 'room', 'think', '6', 'rows', 'of', '50', 'seats', 'in', 'a', 'semi', 'circle', 'and', 'another', '500', 'or', 'so', 'people', 'had', 'to', 'sit', 'in', 'an', 'auditorium', 'and', 'watch', 'her', 'speech', 'on', 'tv', 'they', 'traveled', 'from', 'all', 'over', 'and', 'had', 'to', 'watch', 'her', 'on', 'tv', 'so', 'fucking', 'sad', 'this', 'is', 'fake', 'news', \"hillary's\", 'rallies', 'were', 'small', 'on', 'purpose', 'so', 'she', 'could', 'bum', 'a', 'smoke', 'off', 'you', 'bernie', 'was', 'not', 'able', 'to', 'explain', 'them', 'because', 'black', 'lives', 'matter', 'did', 'not', 'give', 'him', 'permission', 'to', 'speak', 'then', 'she', 'should', 'have', 'called', 'them', 'intimate', 'gatherings', 'not', 'a', 'rally', 'what', 'a', 'loser', 'my', 'car', \"doesn't\", 'drive', 'because', \"that's\", 'one', 'of', 'its', 'safety', 'features', 'since', 'i', \"can't\", 'go', 'anywhere', 'i', \"can't\", 'get', 'in', 'a', 'car', 'accident', 'i', 'really', 'appreciate', 'that', 'the', 'manufacturer', 'had', 'my', 'safety', 'in', 'mind', 'other', 'manufacturers', 'make', 'cars', 'that', 'function', \"that's\", 'too', 'dangerous', 'i', 'heard', 'people', 'say', 'that', 'too', 'are', 'they', 'really', 'that', 'stupid', 'or', 'just', 'trying', 'to', 'downplay', 'her', 'flop', 'ass', 'rallies', 'the', 'hillary', 'voters', 'must', 'have', 'wanted', 'to', 'attend', 'but', 'knew', 'she', 'was', 'going', 'to', 'answer', 'questions', 'and', 'that', 'space', 'would', 'be', 'limited', 'because', 'they', 'are', 'so', 'virtuous', 'thousands', 'stayed', 'home', 'sanctimoniously', 'in', 'cities', 'across', 'the', 'country', 'one', 'can', 'assume', 'the', 'same', 'would', 'have', 'occurred', 'in', 'pennsylvania', 'michigan', 'and', 'wisconsin', 'had', 'she', 'chosen', 'to', 'visit', 'she', 'answered', 'questions', 'i', \"don't\", 'think', 'so', 'she', 'answered', 'scripted', 'questions', 'from', 'the', 'press', 'and', 'started', 'shaking', 'uncontrollably', 'omg', 'those', 'poor', 'people', 'are', 'so', 'delusional', 'yes', 'absolutely', 'hillary', 'wanted', 'small', 'crowds', 'so', 'she', 'could', 'connect', 'to', 'people', 'with', 'her', 'cold', 'robotic', 'ways', 'and', 'answer', 'their', 'important', 'questions', 'the', 'trump', 'rallies', 'packed', 'the', 'venues', 'though', 'thousands', 'of', 'people', 'who', 'all', 'got', 'to', 'see', 'the', 'man', 'himself', 'in', 'front', 'of', 'them', 'super', 'impressive', 'i', 'went', 'to', 'one', 'of', 'hillary’s', 'rallies', 'to', 'see', 'the', 'debacle', 'for', 'myself', 'she', 'didn’t', 'answer', 'a', 'damn', 'question', 'not', 'one', 'a', 'simple', '30m', 'speech', 'to', 'approximately', '300', 'people', 'in', 'a', 'small', 'gym', 'room', 'think', '6', 'rows', 'of', '50', 'seats', 'in', 'a', 'semi', 'circle', 'and', 'another', '500', 'or', 'so', 'people', 'had', 'to', 'sit', 'in', 'an', 'auditorium', 'and', 'watch', 'her', 'speech', 'on', 'tv', 'they', 'traveled', 'from', 'all', 'over', 'and', 'had', 'to', 'watch', 'her', 'on', 'tv', 'so', 'fucking', 'sad', 'this', 'is', 'fake', 'news', \"hillary's\", 'rallies', 'were', 'small', 'on', 'purpose', 'so', 'she', 'could', 'bum', 'a', 'smoke', 'off', 'you', 'bernie', 'was', 'not', 'able', 'to', 'explain', 'them', 'because', 'black', 'lives', 'matter', 'did', 'not', 'give', 'him', 'permission', 'to', 'speak', 'then', 'she', 'should', 'have', 'called', 'them', 'intimate', 'gatherings', 'not', 'a', 'rally', 'what', 'a', 'loser', 'my', 'car', \"doesn't\", 'drive', 'because', \"that's\", 'one', 'of', 'its', 'safety', 'features', 'since', 'i', \"can't\", 'go', 'anywhere', 'i', \"can't\", 'get', 'in', 'a', 'car', 'accident', 'i', 'really', 'appreciate', 'that', 'the', 'manufacturer', 'had', 'my', 'safety', 'in', 'mind', 'other', 'manufacturers', 'make', 'cars', 'that', 'function', \"that's\", 'too', 'dangerous', 'i', 'heard', 'people', 'say', 'that', 'too', 'are', 'they', 'really', 'that', 'stupid', 'or', 'just', 'trying', 'to', 'downplay', 'her', 'flop', 'ass', 'rallies', 'the', 'hillary', 'voters', 'must', 'have', 'wanted', 'to', 'attend', 'but', 'knew', 'she', 'was', 'going', 'to', 'answer', 'questions', 'and', 'that', 'space', 'would', 'be', 'limited', 'because', 'they', 'are', 'so', 'virtuous', 'thousands', 'stayed', 'home', 'sanctimoniously', 'in', 'cities', 'across', 'the', 'country', 'one', 'can', 'assume', 'the', 'same', 'would', 'have', 'occurred', 'in', 'pennsylvania', 'michigan', 'and', 'wisconsin', 'had', 'she', 'chosen', 'to', 'visit', 'she', 'answered', 'questions', 'i', \"don't\", 'think', 'so', 'she', 'answered', 'scripted', 'questions', 'from', 'the', 'press', 'and', 'started', 'shaking', 'uncontrollably', 'omg', 'those', 'poor', 'people', 'are', 'so', 'delusional', 'yes', 'absolutely', 'hillary', 'wanted', 'small', 'crowds', 'so', 'she', 'could', 'connect', 'to', 'people', 'with', 'her', 'cold', 'robotic', 'ways', 'and', 'answer', 'their', 'important', 'questions', 'said', 'the', 'guy', 'on', 'twitter', 'all', 'these', 'services', 'are', 'the', 'same', 'garbage', 'wrapped', 'in', 'different', 'shiny', 'tinfoil', 'some', \"can't\", 'delete', \"it's\", 'an', 'addiction', 'had', 'and', 'old', 'friend', 'over', 'the', 'other', 'night', 'for', 'dinner', 'on', 'facebook', 'every', '5', 'min', 'i', 'asked', 'them', 'not', 'to', 'do', 'that', 'during', 'dinner', \"couldn't\", 'stop', 'it', 'was', 'wierd', 'well', 'considering', 'everyone', 'in', 'the', 'swamp', 'that', 'needs', 'to', 'be', 'mass', 'arrested', 'i', 'think', \"that's\", 'entirely', 'reasonable', 'i', 'tried', 'to', 'say', 'the', 'same', 'my', 'friend', 'but', 'i', 'was', 'a', 'bit', 'adversarial', 'about', 'it', 'we', 'are', 'still', 'leaving', 'huge', 'paper', 'trails', 'of', 'a', 'bill', 'no', 'one', 'read', 'except', 'the', 'lines', 'that', 'matter', 'to', 'them', 'we', 'should', 'be', 'demanding', 'shorter', 'and', 'less', 'legalise', 'document', 'that', 'can', 'be', 'easily', 'scrolled', 'from', 'a', 'secured', 'blackberry', 'or', 'tablet', 'the', 'paper', 'trail', 'can', 'start', 'after', 'its', 'been', 'read', 'and', 'understood', 'by', 'all', 'parties', 'involved', 'the', 'iphone', 'and', 'microsoft', 'word', 'and', 'the', 'like', 'are', 'the', 'only', 'thing', 'the', 'us', 'needs', 'to', 'promote', 'democracy', 'across', 'the', 'world', 'the', 'innovation', 'and', 'creation', 'of', 'great', 'products', 'from', 'a', 'free', 'and', 'open', 'society', 'is', 'like', 'crack', 'cocaine', 'to', 'shit', 'hole', 'commie', 'countries', 'works', 'better', 'than', 'any', '200', 'million', 'dollar', 'campaign', 'engineered', 'by', 'congress', 'best', 'of', 'all', \"it's\", 'free', 'to', 'the', 'us', 'tax', 'payer', 'let', 'our', 'market', 'do', \"it's\", 'thing', 'spend', 'that', '200', 'mil', 'in', 'country', 'that', 'said', 'we', 'have', 'to', 'take', 'trump', 'at', 'his', 'word', 'and', 'i', 'trust', 'him', 'that', 'he', 'won’t', 'sign', 'another', 'garbage', 'bill', 'like', 'this', 'again', 'but', 'it', 'was', 'needed', 'for', 'national', 'security', 'no', 'doubt', 'the', 'establishment', 'will', 'try', 'to', 'pull', 'this', 'again', 'but', 'gotta', 'have', 'faith', 'that', 'trump', 'will', 'also', 'reject', 'autists', 'digging', 'up', 'some', 'really', 'interesting', 'language', 'in', 'the', 'bill', '270', 'million', 'for', 'trialstens', 'of', 'millions', 'for', 'buildings', 'holding', 'people', 'for', 'trialssorry', \"i'm\", 'at', 'work', 'or', 'would', 'linkedit', 'for', 'language', 'this', 'is', 'money', 'for', 'expenses', 'of', 'witnesses', 'imo', 'this', 'is', 'better', 'than', 'for', 'trials', \"that's\", 'a', 'lot', 'of', 'witnesses', 'also', 'quit', 'chimping', 'out', 'omnibus', 'is', 'not', 'a', 'budget', 'obama', 'cared', 'fuckall', 'about', 'what', 'the', 'omnibus', 'said', 'and', 'did', 'what', 'he', 'wanted', 'you', 'think', 'geotus', \"doesn't\", 'know', 'this', 'and', \"won't\", 'do', 'the', 'same', 'one', 'last', 'thing', 'the', 'fucking', 'gop', 'and', 'the', 'dems', 'held', 'our', 'military', 'hostage', 'to', 'enrich', 'their', 'fucking', 'donors', 'and', 'lobbyists', 'and', \"we're\", 'pissed', 'at', 'trump', 'are', 'we', 'going', 'to', 'curl', 'up', 'and', 'die', 'at', 'the', 'first', 'defeat', 'we', \"aren't\", \"pede's\", 'if', \"that's\", 'the', 'case', 'hollow', 'fangs', 'maga', 'burn', 'em', 'all', 'down', 'i', 'voted', 'for', 'trump', 'not', 'republicans', 'why', 'sign', 'this', 'pos', 'bill', 'ryan', 'and', 'mcconnell', 'are', 'determined', 'to', 'be', 'a', 'minority', 'party', 'again', 'it', 'seems', 'do', 'they', 'get', 'more', 'money', 'that', 'way', 'it', 'isn’t', 'as', 'if', 'the', 'fascist', 'leftists', 'will', 'treat', 'you', 'any', 'differently', 'when', 'they', 'gain', 'power', 'imagine', 'another', 'corrupt', 'fbi', 'cia', 'coming', 'after', 'you', 'now', 'a', 'large', 'portion', 'of', 'our', 'deficit', 'is', 'so', 'obvious', 'in', 'just', 'what', 'rand', 'tweeted', 'about', 'the', 'wasteful', 'hideous', 'teasonous', 'spending', 'of', 'our', 'congress', 'is', 'insaannneee', 'seriously', 'church', 'is', 'familiar', 'and', 'because', 'of', 'that', \"it's\", 'comfortable', 'even', 'though', 'you', 'have', 'reasons', 'why', 'you', \"don't\", 'want', 'to', 'go', 'any', 'more', \"we're\", 'loathe', 'to', 'change', 'but', 'since', 'change', 'happens', 'anyway', 'you', 'have', 'a', 'choice', 'you', 'can', 'drive', 'the', 'change', 'in', 'your', 'life', 'or', 'you', 'can', 'let', 'it', 'drive', 'you', 'whether', 'or', 'not', 'anyone', 'thinks', \"it's\", 'rude', 'your', 'participation', 'is', 'a', 'choice', 'and', 'not', 'an', 'obligation', 'you', 'should', 'never', 'fell', 'compelled', 'to', 'participate', 'more', 'than', \"you're\", 'comfortable', 'with', 'feel', 'free', 'to', 'leave', 'the', 'minute', 'you', 'feel', 'the', 'line', 'has', 'been', 'crossed', 'poor', 'conservatives', 'they', 'hold', 'majorities', 'in', 'both', 'the', 'house', 'and', 'the', 'senate', 'and', 'yet', 'somehow', 'they', 'just', \"couldn't\", 'pass', 'a', 'bill', 'that', 'catered', 'to', 'their', 'fantasies', 'the', 'good', 'news', 'is', 'that', 'his', 'time', 'is', 'limited', 'before', 'too', 'long', \"you'll\", 'be', 'finished', 'with', 'school', 'and', 'supporting', 'yourself', 'he', \"can't\", 'do', 'anything', 'about', 'your', 'choices', 'at', 'that', 'point', 'they', 'have', 'nukes', 'and', 'are', 'unstable', 'they', 'have', 'more', 'power', 'than', 'any', 'rivaling', 'group', 'you', 'can', 'think', 'of', 'and', 'are', 'divorced', 'from', 'logic', 'saying', 'they', 'are', 'dangerous', 'is', 'an', 'understatement', 'they', 'could', 'destroy', 'the', 'world', 'in', 'ways', 'no', 'other', 'group', 'of', 'that', 'mentality', 'could', 'which', 'would', 'be', 'absolutely', 'no', 'one', 'left', 'yeah', 'it’s', 'not', 'simply', 'rhetorical', 'license', 'to', 'say', 'they', 'are', 'the', 'most', 'dangerous', 'in', 'history', 'academically', 'speaking', 'the', 'most', 'dangerous', 'would', 'have', 'to', 'be', 'conditionally', 'defined', 'and', 'i', 'doubt', 'the', 'republican', 'party', 'caused', 'the', 'most', 'deaths', 'there', 'are', 'a', 'ton', 'more', 'organizations', 'i', 'would', 'consider', 'to', 'be', 'more', 'lethal', 'republicans', 'are', 'just', 'greedy', 'rich', 'people', 'as', 'much', 'as', 'i', 'respect', 'chomsky', 'he’s', 'usually', 'known', 'to', 'exaggerate', 'his', 'conclusions', 'i', 'definitely', 'agree', 'that', 'the', 'republican', 'party', 'is', 'dangerous', 'but', 'to', 'call', 'it', 'the', 'most', 'dangerous', 'organization', 'is', 'quite', 'a', 'stretch', 'i', 'can', 'not', 'comprehend', 'that', 'people', 'are', 'sitting', 'here', 'agreeing', 'with', 'this', 'if', 'this', 'is', 'how', 'people', 'think', 'about', 'people', 'with', 'different', 'political', 'views', 'now', 'we', 'are', 'all', 'truly', 'doomed', 'the', 'funniest', 'part', 'is', 'saying', 'this', 'is', 'the', 'last', 'time', \"i'll\", 'sign', 'a', 'bill', 'without', 'having', 'time', 'to', 'read', 'it', 'after', 'signing', 'a', 'tax', 'bill', 'that', \"wasn't\", 'even', 'written', 'at', 'the', 'time', 'of', 'signing', 'also', 'knowing', 'that', 'he', 'has', 'to', 'have', 'his', 'morning', 'briefings', 'reduced', 'even', 'more', 'and', 'read', 'to', 'him', 'instead', 'of', 'spending', 'twenty', 'to', 'seventy', 'billion', 'on', 'trump’s', 'dumb', 'wall…', 'why', 'doesn’t', 'trump', 'spend', 'twenty', 'to', 'seventy', 'billion', 'on', 'housing', 'for', 'homeless', 'veterans', 'faux', '’news’', 'fox', 'reporting', 'is', 'just', 'there', 'to', 'steer', 'people', 'to', 'the', 'less', 'factual', 'commentary', 'by', 'their', 'personalities', 'they', 'also', 'give', 'little', 'attention', 'to', 'stories', 'that', 'hurt', 'them', 'elevate', 'stories', 'that', 'frighten', 'or', 'anger', 'the', 'audience', 'about', 'their', 'political', 'scapegoats', 'and', 'frequently', 'give', 'reporting', 'time', 'to', 'stories', 'they', 'know', 'are', 'false', 'in', 'order', 'to', 'plant', 'them', 'with', 'the', 'audience', 'before', 'eventually', 'retracting', 'them', 'days', 'later']"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#load Training Data\n",
    "\n",
    "\n",
    "import glob, os\n",
    "from keras.preprocessing import text\n",
    "\n",
    "def getStrings(path):\n",
    "    strings = []\n",
    "    for filename in glob.glob(os.path.join(path, '*.txt')):\n",
    "        # do your stuff\n",
    "        with open(filename) as f:\n",
    "            lines = f.read().replace('\\n', '')\n",
    "            lines.lower()\n",
    "            lines.split()\n",
    "            result = text.text_to_word_sequence(lines)\n",
    "            strings.append(result)\n",
    "    return strings\n",
    "\n",
    "testconpath = 'Data\\\\Test\\\\Cons'\n",
    "testlibpath = 'Data\\\\Test\\\\Lib'\n",
    "trainconpath = 'Data\\\\Train\\\\Cons'\n",
    "trainlibpath = 'Data\\\\Train\\\\Cons'\n",
    "\n",
    "x_stringtrain = getStrings(trainconpath)\n",
    "y_stringtrain = getStrings(trainlibpath)\n",
    "x_stringtest = getStrings(testconpath)\n",
    "y_stringtest = getStrings(testlibpath)\n",
    "\n",
    "stringlist = x_stringtrain\n",
    "stringlist.append(y_stringtrain)\n",
    "stringlist.append(x_stringtest)\n",
    "stringlist.append(y_stringtest)\n",
    "\n",
    "wordlist = ['word', 'another']\n",
    "\n",
    "for lists in stringlist:\n",
    "    if(isinstance(lists, list)):\n",
    "        for sub in lists:\n",
    "            if(isinstance(sub, list)):\n",
    "                for subsub in sub:\n",
    "                    if(isinstance(subsub, str)):\n",
    "                        wordlist.append(subsub)\n",
    "            elif(isinstance(sub, str)):\n",
    "                wordlist.append(sub)\n",
    "    elif(isinstance(lists, str)):\n",
    "            wordlist.append(lists)\n",
    "        \n",
    "print(wordlist)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make stringlist to int array with each value equalling an int\n",
    "vocab = [(1, \"first\")]\n",
    "vocab.append((2, \"second\"))\n",
    "count = 3\n",
    "for word in wordlist:\n",
    "    added = False\n",
    "    for entry in vocab:\n",
    "        if(entry[1] == word):\n",
    "            added = True\n",
    "    if(added == False):\n",
    "        if(isinstance(word, str)):\n",
    "            vocab.append((count, word))\n",
    "            count = count+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Train and Test lists to integer\n",
    "def stringtointarr(lists):\n",
    "    intarr = []\n",
    "    for strings in lists:\n",
    "        for string in strings:\n",
    "            subintarr = []\n",
    "            for entry in vocab:\n",
    "                if(string == entry[1]):\n",
    "                    subintarr.append(entry[0])\n",
    "            intarr.append(subintarr)\n",
    "    return(intarr)\n",
    "    \n",
    "x_longtrain = stringtointarr(x_stringtrain)\n",
    "y_longtrain = stringtointarr(y_stringtrain)\n",
    "x_longtest = stringtointarr(x_stringtest)\n",
    "y_longtest = stringtointarr(y_stringtest)\n",
    "\n",
    "x_train = []\n",
    "y_train=[]\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "#make x_train and y_train the same length\n",
    "length = 288\n",
    "\n",
    "def limitsize(arr, length):\n",
    "    limited = []\n",
    "    for entries in arr:\n",
    "        if(length>0):\n",
    "            limited.append(entries)\n",
    "            length = length-1\n",
    "        else:\n",
    "            break\n",
    "    return limited\n",
    "\n",
    "x_train = limitsize(x_longtrain, length)\n",
    "y_train = limitsize(y_longtrain, length)\n",
    "x_testlist = limitsize(x_longtest, length)\n",
    "y_testlist = limitsize(y_longtest, length)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 train sequences\n",
      "0 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (288, 100)\n",
      "x_test shape: (0, 100)\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Epoch 1/2\n",
      "288/288 [==============================] - 1s 2ms/step - loss: -1173.0385 - acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "288/288 [==============================] - 1s 2ms/step - loss: -1173.0385 - acc: 0.0000e+00\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-e20a07f676d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlisttonumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_testlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlisttonumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_testlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-e20a07f676d2>\u001b[0m in \u001b[0;36mlisttonumpy\u001b[1;34m(arr)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0msubnump\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0marrs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlists\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubnump\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubnump\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "model.fit(np.array(x_train),np.array(y_train), epochs=2, batch_size=10)\n",
    "\n",
    "def listtonumpy(arr):\n",
    "    numpy = np.array([])\n",
    "    for lists in arr:\n",
    "        subnump = np.array([])\n",
    "        for arrs in lists:\n",
    "            np.concatenate(subnump, np.array(arrs))\n",
    "        np.concatenate(numpy, subnump)\n",
    "    return numpy\n",
    "\n",
    "x_test = listtonumpy(x_testlist)\n",
    "y_test = listtonumpy(y_testlist)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
