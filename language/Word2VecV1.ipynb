{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from string import punctuation\n",
    "\n",
    "DICTIONARYSIZE = 100\n",
    "SKIPWINDOW = 2\n",
    "DATATEXT = 'ham.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(input_dim=DICTIONARYSIZE, units = DICTIONARYSIZE//40, activation = \"relu\"))\n",
    "model.add(Dense(units = DICTIONARYSIZE, activation = \"softmax\"))\n",
    "\n",
    "sgd = SGD(lr = 0.1)\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "             optimizer = sgd,\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['favour', 'fingers', \"utter'd\", 'roses', 'return', 'tend', 'us--thou', 'shook', \"where's\", 'hush']\n"
     ]
    }
   ],
   "source": [
    "#Clean data set\n",
    "def clean(lines):\n",
    "    ret = []\n",
    "    count = {}\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        split = line.split()\n",
    "        if(len(split) == 0): continue\n",
    "        #This is specific to the Hamlet data set. Add code for lins to disregard\n",
    "        if(split[0].isupper()): continue\n",
    "        for word in split:\n",
    "            word = word.lower()\n",
    "            if word[-1] in punctuation:\n",
    "                word = word[:-1]\n",
    "            if len(word) == 0: continue\n",
    "            if word[0] in punctuation:\n",
    "                word = word[1:]\n",
    "            if len(word) == 0: continue\n",
    "            ret.append(word)\n",
    "            if word in count.keys():\n",
    "                count[word] += 1\n",
    "            else:\n",
    "                count[word] = 1\n",
    "            \n",
    "    return ret, count\n",
    "\n",
    "lines = open(DATATEXT,'r').readlines()\n",
    "wordList, count = clean(lines)\n",
    "\n",
    "indexToWord = list(set(wordList))\n",
    "print(indexToWord[:10])\n",
    "\n",
    "#Construct word-to-index dictionary\n",
    "wordToIndex = {indexToWord[i]:i for i in range(len(indexToWord))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Duplication test)\n",
    "for i in range(-100,-1):\n",
    "    if (i%len(indexToWord)) != wordToIndex[indexToWord[i]]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct word-to-vector (one-hot) and vector-to-word (one-hot) dictionary\n",
    "#The \"hot\" one will be indexed by popularity\n",
    "wordToVector = {}\n",
    "zeroVector = [0 for _ in range(DICTIONARYSIZE)]\n",
    "#First sort word-to-index by word popularity\n",
    "wordsByPopularityDoubles = sorted(count.items(), key = lambda x:x[1], reverse = True)\n",
    "wordsByPopularity = [wordsByPopularityDoubles[i][0] for i in range(len(wordsByPopularityDoubles))]\n",
    "\n",
    "for i in range(DICTIONARYSIZE):\n",
    "    vec = zeroVector.copy()\n",
    "    vec[i] = 1\n",
    "    wordToVector[wordsByPopularity[i]] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most popular words:\n",
      "['the', 'and', 'to', 'of', 'you', 'my', 'a', 'in', 'it', 'i', 'that', 'is', 'his', 'not', 'this', 'with', 'but', 'for', 'your']\n",
      "4719 words total.\n"
     ]
    }
   ],
   "source": [
    "print(\"Most popular words:\\n\" + str(wordsByPopularity[:19]))\n",
    "print(str(len(wordsByPopularity)) + \" words total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training arrays (Skip-gram)\n",
    "#Currently creates the same batch every call\n",
    "#May be stochastic in the future\n",
    "def generate_batch():\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    #Start the moving window. Windex is the index of the center of the window.\n",
    "    #OPTIMIZE: Use np arrays\n",
    "    #OPTIMIZE: Implement mini-batch features shown in the word2vec tutorial\n",
    "    for windex in range(SKIPWINDOW, len(wordList) - SKIPWINDOW):\n",
    "        centerWord = wordList[windex]\n",
    "        #OPTIMIZE replace this lookup using a nice dictionary OOH OR A NUMPY ARRAY BOOLEAN THING\n",
    "        if not (centerWord in wordToVector.keys()): continue\n",
    "        for contex in range(-SKIPWINDOW, SKIPWINDOW):\n",
    "            contextWord = wordList[contex]\n",
    "            #Skip the center word (it is a skip-gram after all...)\n",
    "            if contex == windex: continue\n",
    "            #OPTIMIZE replace this lookup using a nice dictionary\n",
    "            if not (contextWord in wordToVector.keys()): continue\n",
    "            x_train.append(wordToVector[wordList[windex]])\n",
    "            y_train.append(wordToVector[wordList[contex]])\n",
    "    return np.array(x_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "15491/15491 [==============================] - 1s 44us/step - loss: 4.6317 - acc: 0.0000e+00\n",
      "Epoch 2/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 4.5296 - acc: 0.7853\n",
      "Epoch 3/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 4.4280 - acc: 1.0000\n",
      "Epoch 4/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 4.3271 - acc: 1.0000\n",
      "Epoch 5/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 4.2267 - acc: 1.0000\n",
      "Epoch 6/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 4.1269 - acc: 1.0000\n",
      "Epoch 7/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 4.0276 - acc: 1.0000\n",
      "Epoch 8/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 3.9289 - acc: 1.0000\n",
      "Epoch 9/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 3.8308 - acc: 1.0000\n",
      "Epoch 10/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 3.7331 - acc: 1.0000\n",
      "Epoch 11/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 3.6360 - acc: 1.0000\n",
      "Epoch 12/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 3.5394 - acc: 1.0000\n",
      "Epoch 13/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 3.4437 - acc: 1.0000\n",
      "Epoch 14/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 3.3484 - acc: 1.0000\n",
      "Epoch 15/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 3.2539 - acc: 1.0000\n",
      "Epoch 16/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 3.1601 - acc: 1.0000\n",
      "Epoch 17/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 3.0671 - acc: 1.0000\n",
      "Epoch 18/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 2.9749 - acc: 1.0000\n",
      "Epoch 19/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 2.8837 - acc: 1.0000\n",
      "Epoch 20/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 2.7935 - acc: 1.0000\n",
      "Epoch 21/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 2.7042 - acc: 1.0000\n",
      "Epoch 22/2000\n",
      "15491/15491 [==============================] - 0s 28us/step - loss: 2.6162 - acc: 1.0000\n",
      "Epoch 23/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 2.5294 - acc: 1.0000\n",
      "Epoch 24/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 2.4439 - acc: 1.0000\n",
      "Epoch 25/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 2.3596 - acc: 1.0000\n",
      "Epoch 26/2000\n",
      "15491/15491 [==============================] - 0s 32us/step - loss: 2.2769 - acc: 1.0000\n",
      "Epoch 27/2000\n",
      "15491/15491 [==============================] - 0s 31us/step - loss: 2.1957 - acc: 1.0000\n",
      "Epoch 28/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 2.1161 - acc: 1.0000\n",
      "Epoch 29/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 2.0381 - acc: 1.0000\n",
      "Epoch 30/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 1.9618 - acc: 1.0000\n",
      "Epoch 31/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 1.8874 - acc: 1.0000\n",
      "Epoch 32/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 1.8149 - acc: 1.0000\n",
      "Epoch 33/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 1.7444 - acc: 1.0000\n",
      "Epoch 34/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 1.6759 - acc: 1.0000\n",
      "Epoch 35/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 1.6094 - acc: 1.0000\n",
      "Epoch 36/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 1.5451 - acc: 1.0000\n",
      "Epoch 37/2000\n",
      "15491/15491 [==============================] - 0s 30us/step - loss: 1.4828 - acc: 1.0000\n",
      "Epoch 38/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 1.4227 - acc: 1.0000\n",
      "Epoch 39/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 1.3648 - acc: 1.0000\n",
      "Epoch 40/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 1.3091 - acc: 1.0000\n",
      "Epoch 41/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 1.2555 - acc: 1.0000\n",
      "Epoch 42/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 1.2041 - acc: 1.0000\n",
      "Epoch 43/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 1.1548 - acc: 1.0000\n",
      "Epoch 44/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 1.1076 - acc: 1.0000\n",
      "Epoch 45/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 1.0625 - acc: 1.0000\n",
      "Epoch 46/2000\n",
      "15491/15491 [==============================] - 0s 28us/step - loss: 1.0194 - acc: 1.0000\n",
      "Epoch 47/2000\n",
      "15491/15491 [==============================] - 0s 29us/step - loss: 0.9782 - acc: 1.0000\n",
      "Epoch 48/2000\n",
      "15491/15491 [==============================] - 0s 28us/step - loss: 0.9390 - acc: 1.0000\n",
      "Epoch 49/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.9016 - acc: 1.0000\n",
      "Epoch 50/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.8660 - acc: 1.0000\n",
      "Epoch 51/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.8321 - acc: 1.0000\n",
      "Epoch 52/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.7999 - acc: 1.0000\n",
      "Epoch 53/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.7692 - acc: 1.0000\n",
      "Epoch 54/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.7401 - acc: 1.0000\n",
      "Epoch 55/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.7124 - acc: 1.0000\n",
      "Epoch 56/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.6861 - acc: 1.0000\n",
      "Epoch 57/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.6610 - acc: 1.0000\n",
      "Epoch 58/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.6373 - acc: 1.0000\n",
      "Epoch 59/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.6146 - acc: 1.0000\n",
      "Epoch 60/2000\n",
      "15491/15491 [==============================] - 0s 29us/step - loss: 0.5931 - acc: 1.0000\n",
      "Epoch 61/2000\n",
      "15491/15491 [==============================] - 0s 28us/step - loss: 0.5727 - acc: 1.0000\n",
      "Epoch 62/2000\n",
      "15491/15491 [==============================] - 0s 28us/step - loss: 0.5532 - acc: 1.0000\n",
      "Epoch 63/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.5347 - acc: 1.0000\n",
      "Epoch 64/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.5171 - acc: 1.0000\n",
      "Epoch 65/2000\n",
      "15491/15491 [==============================] - 0s 30us/step - loss: 0.5004 - acc: 1.0000\n",
      "Epoch 66/2000\n",
      "15491/15491 [==============================] - 0s 28us/step - loss: 0.4844 - acc: 1.0000\n",
      "Epoch 67/2000\n",
      "15491/15491 [==============================] - 0s 29us/step - loss: 0.4692 - acc: 1.0000\n",
      "Epoch 68/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.4547 - acc: 1.0000\n",
      "Epoch 69/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.4409 - acc: 1.0000\n",
      "Epoch 70/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.4277 - acc: 1.0000\n",
      "Epoch 71/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.4151 - acc: 1.0000\n",
      "Epoch 72/2000\n",
      "15491/15491 [==============================] - 0s 30us/step - loss: 0.4030 - acc: 1.0000\n",
      "Epoch 73/2000\n",
      "15491/15491 [==============================] - 0s 29us/step - loss: 0.3915 - acc: 1.0000\n",
      "Epoch 74/2000\n",
      "15491/15491 [==============================] - 0s 30us/step - loss: 0.3804 - acc: 1.0000\n",
      "Epoch 75/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.3699 - acc: 1.0000\n",
      "Epoch 76/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.3597 - acc: 1.0000\n",
      "Epoch 77/2000\n",
      "15491/15491 [==============================] - 0s 28us/step - loss: 0.3500 - acc: 1.0000\n",
      "Epoch 78/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.3406 - acc: 1.0000\n",
      "Epoch 79/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.3317 - acc: 1.0000\n",
      "Epoch 80/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.3231 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.3148 - acc: 1.0000\n",
      "Epoch 82/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.3069 - acc: 1.0000\n",
      "Epoch 83/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.2993 - acc: 1.0000\n",
      "Epoch 84/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.2919 - acc: 1.0000\n",
      "Epoch 85/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.2849 - acc: 1.0000\n",
      "Epoch 86/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.2781 - acc: 1.0000\n",
      "Epoch 87/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.2715 - acc: 1.0000\n",
      "Epoch 88/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.2652 - acc: 1.0000\n",
      "Epoch 89/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.2590 - acc: 1.0000\n",
      "Epoch 90/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.2531 - acc: 1.0000\n",
      "Epoch 91/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.2474 - acc: 1.0000\n",
      "Epoch 92/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.2419 - acc: 1.0000\n",
      "Epoch 93/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.2365 - acc: 1.0000\n",
      "Epoch 94/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.2313 - acc: 1.0000\n",
      "Epoch 95/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.2262 - acc: 1.0000\n",
      "Epoch 96/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.2213 - acc: 1.0000\n",
      "Epoch 97/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.2165 - acc: 1.0000\n",
      "Epoch 98/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.2120 - acc: 1.0000\n",
      "Epoch 99/2000\n",
      "15491/15491 [==============================] - 0s 29us/step - loss: 0.2075 - acc: 1.0000\n",
      "Epoch 100/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.2032 - acc: 1.0000\n",
      "Epoch 101/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.1990 - acc: 1.0000\n",
      "Epoch 102/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.1949 - acc: 1.0000\n",
      "Epoch 103/2000\n",
      "15491/15491 [==============================] - 0s 29us/step - loss: 0.1910 - acc: 1.0000\n",
      "Epoch 104/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.1872 - acc: 1.0000\n",
      "Epoch 105/2000\n",
      "15491/15491 [==============================] - 0s 31us/step - loss: 0.1835 - acc: 1.0000\n",
      "Epoch 106/2000\n",
      "15491/15491 [==============================] - 0s 30us/step - loss: 0.1799 - acc: 1.0000\n",
      "Epoch 107/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.1764 - acc: 1.0000\n",
      "Epoch 108/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.1730 - acc: 1.0000\n",
      "Epoch 109/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.1697 - acc: 1.0000\n",
      "Epoch 110/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.1664 - acc: 1.0000\n",
      "Epoch 111/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.1633 - acc: 1.0000\n",
      "Epoch 112/2000\n",
      "15491/15491 [==============================] - 0s 31us/step - loss: 0.1603 - acc: 1.0000\n",
      "Epoch 113/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.1573 - acc: 1.0000\n",
      "Epoch 114/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.1544 - acc: 1.0000\n",
      "Epoch 115/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.1516 - acc: 1.0000\n",
      "Epoch 116/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.1489 - acc: 1.0000\n",
      "Epoch 117/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.1462 - acc: 1.0000\n",
      "Epoch 118/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.1436 - acc: 1.0000\n",
      "Epoch 119/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.1411 - acc: 1.0000\n",
      "Epoch 120/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.1386 - acc: 1.0000\n",
      "Epoch 121/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.1362 - acc: 1.0000\n",
      "Epoch 122/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.1339 - acc: 1.0000\n",
      "Epoch 123/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.1316 - acc: 1.0000\n",
      "Epoch 124/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.1294 - acc: 1.0000\n",
      "Epoch 125/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.1272 - acc: 1.0000\n",
      "Epoch 126/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.1250 - acc: 1.0000\n",
      "Epoch 127/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.1230 - acc: 1.0000\n",
      "Epoch 128/2000\n",
      "15491/15491 [==============================] - 0s 28us/step - loss: 0.1209 - acc: 1.0000\n",
      "Epoch 129/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.1190 - acc: 1.0000\n",
      "Epoch 130/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.1170 - acc: 1.0000\n",
      "Epoch 131/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.1151 - acc: 1.0000\n",
      "Epoch 132/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.1133 - acc: 1.0000\n",
      "Epoch 133/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.1115 - acc: 1.0000\n",
      "Epoch 134/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.1097 - acc: 1.0000\n",
      "Epoch 135/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.1080 - acc: 1.0000\n",
      "Epoch 136/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.1063 - acc: 1.0000\n",
      "Epoch 137/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.1046 - acc: 1.0000\n",
      "Epoch 138/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.1030 - acc: 1.0000\n",
      "Epoch 139/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.1014 - acc: 1.0000\n",
      "Epoch 140/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0999 - acc: 1.0000\n",
      "Epoch 141/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0984 - acc: 1.0000\n",
      "Epoch 142/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0969 - acc: 1.0000\n",
      "Epoch 143/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0954 - acc: 1.0000\n",
      "Epoch 144/2000\n",
      "15491/15491 [==============================] - 0s 28us/step - loss: 0.0940 - acc: 1.0000\n",
      "Epoch 145/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0926 - acc: 1.0000\n",
      "Epoch 146/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0913 - acc: 1.0000\n",
      "Epoch 147/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0899 - acc: 1.0000\n",
      "Epoch 148/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0886 - acc: 1.0000\n",
      "Epoch 149/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0873 - acc: 1.0000\n",
      "Epoch 150/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0861 - acc: 1.0000\n",
      "Epoch 151/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0849 - acc: 1.0000\n",
      "Epoch 152/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0837 - acc: 1.0000\n",
      "Epoch 153/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0825 - acc: 1.0000\n",
      "Epoch 154/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0813 - acc: 1.0000\n",
      "Epoch 155/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0802 - acc: 1.0000\n",
      "Epoch 156/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0791 - acc: 1.0000\n",
      "Epoch 157/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0780 - acc: 1.0000\n",
      "Epoch 158/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0769 - acc: 1.0000\n",
      "Epoch 159/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0759 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0748 - acc: 1.0000\n",
      "Epoch 161/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0738 - acc: 1.0000\n",
      "Epoch 162/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0728 - acc: 1.0000\n",
      "Epoch 163/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0719 - acc: 1.0000\n",
      "Epoch 164/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0709 - acc: 1.0000\n",
      "Epoch 165/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0700 - acc: 1.0000\n",
      "Epoch 166/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0691 - acc: 1.0000\n",
      "Epoch 167/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0682 - acc: 1.0000\n",
      "Epoch 168/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0673 - acc: 1.0000\n",
      "Epoch 169/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0664 - acc: 1.0000\n",
      "Epoch 170/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0656 - acc: 1.0000\n",
      "Epoch 171/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0647 - acc: 1.0000\n",
      "Epoch 172/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0639 - acc: 1.0000\n",
      "Epoch 173/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0631 - acc: 1.0000\n",
      "Epoch 174/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0623 - acc: 1.0000\n",
      "Epoch 175/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0615 - acc: 1.0000\n",
      "Epoch 176/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0608 - acc: 1.0000\n",
      "Epoch 177/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0600 - acc: 1.0000\n",
      "Epoch 178/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0593 - acc: 1.0000\n",
      "Epoch 179/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0586 - acc: 1.0000\n",
      "Epoch 180/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0579 - acc: 1.0000\n",
      "Epoch 181/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0572 - acc: 1.0000\n",
      "Epoch 182/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0565 - acc: 1.0000\n",
      "Epoch 183/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0558 - acc: 1.0000\n",
      "Epoch 184/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0551 - acc: 1.0000\n",
      "Epoch 185/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0545 - acc: 1.0000\n",
      "Epoch 186/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0539 - acc: 1.0000\n",
      "Epoch 187/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0532 - acc: 1.0000\n",
      "Epoch 188/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0526 - acc: 1.0000\n",
      "Epoch 189/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0520 - acc: 1.0000\n",
      "Epoch 190/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0514 - acc: 1.0000\n",
      "Epoch 191/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0508 - acc: 1.0000\n",
      "Epoch 192/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0502 - acc: 1.0000\n",
      "Epoch 193/2000\n",
      "15491/15491 [==============================] - 0s 29us/step - loss: 0.0497 - acc: 1.0000\n",
      "Epoch 194/2000\n",
      "15491/15491 [==============================] - 0s 29us/step - loss: 0.0491 - acc: 1.0000\n",
      "Epoch 195/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0486 - acc: 1.0000\n",
      "Epoch 196/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0480 - acc: 1.0000\n",
      "Epoch 197/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0475 - acc: 1.0000\n",
      "Epoch 198/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0470 - acc: 1.0000\n",
      "Epoch 199/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0464 - acc: 1.0000\n",
      "Epoch 200/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0459 - acc: 1.0000\n",
      "Epoch 201/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0454 - acc: 1.0000\n",
      "Epoch 202/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0449 - acc: 1.0000\n",
      "Epoch 203/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0445 - acc: 1.0000\n",
      "Epoch 204/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0440 - acc: 1.0000\n",
      "Epoch 205/2000\n",
      "15491/15491 [==============================] - 0s 28us/step - loss: 0.0435 - acc: 1.0000\n",
      "Epoch 206/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0431 - acc: 1.0000\n",
      "Epoch 207/2000\n",
      "15491/15491 [==============================] - 0s 29us/step - loss: 0.0426 - acc: 1.0000\n",
      "Epoch 208/2000\n",
      "15491/15491 [==============================] - 0s 28us/step - loss: 0.0422 - acc: 1.0000\n",
      "Epoch 209/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0417 - acc: 1.0000\n",
      "Epoch 210/2000\n",
      "15491/15491 [==============================] - 0s 28us/step - loss: 0.0413 - acc: 1.0000\n",
      "Epoch 211/2000\n",
      "15491/15491 [==============================] - 0s 29us/step - loss: 0.0409 - acc: 1.0000\n",
      "Epoch 212/2000\n",
      "15491/15491 [==============================] - 0s 29us/step - loss: 0.0404 - acc: 1.0000\n",
      "Epoch 213/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0400 - acc: 1.0000\n",
      "Epoch 214/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0396 - acc: 1.0000\n",
      "Epoch 215/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0392 - acc: 1.0000\n",
      "Epoch 216/2000\n",
      "15491/15491 [==============================] - 0s 29us/step - loss: 0.0388 - acc: 1.0000\n",
      "Epoch 217/2000\n",
      "15491/15491 [==============================] - 0s 28us/step - loss: 0.0384 - acc: 1.0000\n",
      "Epoch 218/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0380 - acc: 1.0000\n",
      "Epoch 219/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0377 - acc: 1.0000\n",
      "Epoch 220/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0373 - acc: 1.0000\n",
      "Epoch 221/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0369 - acc: 1.0000\n",
      "Epoch 222/2000\n",
      "15491/15491 [==============================] - 0s 28us/step - loss: 0.0366 - acc: 1.0000\n",
      "Epoch 223/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0362 - acc: 1.0000\n",
      "Epoch 224/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0359 - acc: 1.0000\n",
      "Epoch 225/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0355 - acc: 1.0000\n",
      "Epoch 226/2000\n",
      "15491/15491 [==============================] - 0s 28us/step - loss: 0.0352 - acc: 1.0000\n",
      "Epoch 227/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0348 - acc: 1.0000\n",
      "Epoch 228/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0345 - acc: 1.0000\n",
      "Epoch 229/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0342 - acc: 1.0000\n",
      "Epoch 230/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0339 - acc: 1.0000\n",
      "Epoch 231/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0335 - acc: 1.0000\n",
      "Epoch 232/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0332 - acc: 1.0000\n",
      "Epoch 233/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0329 - acc: 1.0000\n",
      "Epoch 234/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0326 - acc: 1.0000\n",
      "Epoch 235/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0323 - acc: 1.0000\n",
      "Epoch 236/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0320 - acc: 1.0000\n",
      "Epoch 237/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0317 - acc: 1.0000\n",
      "Epoch 238/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0315 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 239/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0312 - acc: 1.0000\n",
      "Epoch 240/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0309 - acc: 1.0000\n",
      "Epoch 241/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0306 - acc: 1.0000\n",
      "Epoch 242/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0303 - acc: 1.0000\n",
      "Epoch 243/2000\n",
      "15491/15491 [==============================] - 0s 30us/step - loss: 0.0301 - acc: 1.0000\n",
      "Epoch 244/2000\n",
      "15491/15491 [==============================] - 0s 29us/step - loss: 0.0298 - acc: 1.0000\n",
      "Epoch 245/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0295 - acc: 1.0000\n",
      "Epoch 246/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0293 - acc: 1.0000\n",
      "Epoch 247/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0290 - acc: 1.0000\n",
      "Epoch 248/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0288 - acc: 1.0000\n",
      "Epoch 249/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0285 - acc: 1.0000\n",
      "Epoch 250/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0283 - acc: 1.0000\n",
      "Epoch 251/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0281 - acc: 1.0000\n",
      "Epoch 252/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0278 - acc: 1.0000\n",
      "Epoch 253/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0276 - acc: 1.0000\n",
      "Epoch 254/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0273 - acc: 1.0000\n",
      "Epoch 255/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0271 - acc: 1.0000\n",
      "Epoch 256/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0269 - acc: 1.0000\n",
      "Epoch 257/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0267 - acc: 1.0000\n",
      "Epoch 258/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0265 - acc: 1.0000\n",
      "Epoch 259/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0262 - acc: 1.0000\n",
      "Epoch 260/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0260 - acc: 1.0000\n",
      "Epoch 261/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0258 - acc: 1.0000\n",
      "Epoch 262/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0256 - acc: 1.0000\n",
      "Epoch 263/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0254 - acc: 1.0000\n",
      "Epoch 264/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0252 - acc: 1.0000\n",
      "Epoch 265/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0250 - acc: 1.0000\n",
      "Epoch 266/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0248 - acc: 1.0000\n",
      "Epoch 267/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0246 - acc: 1.0000\n",
      "Epoch 268/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0244 - acc: 1.0000\n",
      "Epoch 269/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0242 - acc: 1.0000\n",
      "Epoch 270/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0240 - acc: 1.0000\n",
      "Epoch 271/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0238 - acc: 1.0000\n",
      "Epoch 272/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0236 - acc: 1.0000\n",
      "Epoch 273/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0235 - acc: 1.0000\n",
      "Epoch 274/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0233 - acc: 1.0000\n",
      "Epoch 275/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0231 - acc: 1.0000\n",
      "Epoch 276/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0229 - acc: 1.0000\n",
      "Epoch 277/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0228 - acc: 1.0000\n",
      "Epoch 278/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0226 - acc: 1.0000\n",
      "Epoch 279/2000\n",
      "15491/15491 [==============================] - 0s 25us/step - loss: 0.0224 - acc: 1.0000\n",
      "Epoch 280/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0222 - acc: 1.0000\n",
      "Epoch 281/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0221 - acc: 1.0000\n",
      "Epoch 282/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0219 - acc: 1.0000\n",
      "Epoch 283/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0217 - acc: 1.0000\n",
      "Epoch 284/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0216 - acc: 1.0000\n",
      "Epoch 285/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0214 - acc: 1.0000\n",
      "Epoch 286/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0213 - acc: 1.0000\n",
      "Epoch 287/2000\n",
      "15491/15491 [==============================] - 0s 27us/step - loss: 0.0211 - acc: 1.0000\n",
      "Epoch 288/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0210 - acc: 1.0000\n",
      "Epoch 289/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0208 - acc: 1.0000\n",
      "Epoch 290/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0207 - acc: 1.0000\n",
      "Epoch 291/2000\n",
      "15491/15491 [==============================] - 0s 26us/step - loss: 0.0205 - acc: 1.0000\n",
      "Epoch 292/2000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-e26f5e656992>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\brantley\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\brantley\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\brantley\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brantley\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brantley\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brantley\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brantley\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brantley\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brantley\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_train, y_train = generate_batch()\n",
    "model.fit(x_train, y_train, epochs = 2000, batch_size = len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.5%: there\n",
      "98.4%: there\n",
      "98.30000000000001%: there\n",
      "98.30000000000001%: there\n",
      "97.4%: there\n",
      "98.0%: there\n",
      "98.0%: there\n",
      "97.7%: there\n",
      "97.4%: there\n",
      "97.7%: there\n",
      "97.60000000000001%: there\n",
      "97.4%: there\n",
      "97.60000000000001%: there\n",
      "97.7%: there\n",
      "98.10000000000001%: there\n",
      "98.30000000000001%: there\n",
      "98.4%: there\n",
      "98.10000000000001%: there\n",
      "98.10000000000001%: there\n",
      "97.9%: there\n",
      "98.2%: there\n",
      "97.9%: there\n",
      "97.60000000000001%: there\n",
      "97.4%: there\n",
      "97.60000000000001%: there\n",
      "98.30000000000001%: there\n",
      "98.10000000000001%: there\n",
      "97.80000000000001%: there\n",
      "97.80000000000001%: there\n",
      "98.30000000000001%: there\n",
      "97.5%: there\n",
      "97.80000000000001%: there\n",
      "98.0%: there\n",
      "97.4%: there\n",
      "98.30000000000001%: there\n",
      "97.4%: there\n",
      "97.30000000000001%: there\n",
      "97.80000000000001%: there\n",
      "97.80000000000001%: there\n",
      "98.2%: there\n",
      "97.4%: there\n",
      "97.60000000000001%: there\n",
      "98.2%: there\n",
      "97.60000000000001%: there\n",
      "98.30000000000001%: there\n",
      "97.4%: there\n",
      "97.60000000000001%: there\n",
      "97.9%: there\n",
      "98.30000000000001%: there\n",
      "97.5%: there\n",
      "97.9%: there\n",
      "98.0%: there\n",
      "98.2%: there\n",
      "98.30000000000001%: there\n",
      "97.4%: there\n",
      "98.2%: there\n",
      "98.10000000000001%: there\n",
      "98.2%: there\n",
      "97.4%: there\n",
      "97.80000000000001%: there\n",
      "97.5%: there\n",
      "98.30000000000001%: there\n",
      "97.4%: there\n",
      "97.9%: there\n",
      "98.2%: there\n",
      "97.4%: there\n",
      "97.9%: there\n",
      "98.0%: there\n",
      "97.60000000000001%: there\n",
      "97.4%: there\n",
      "98.2%: there\n",
      "98.30000000000001%: there\n",
      "98.0%: there\n",
      "98.10000000000001%: there\n",
      "97.7%: there\n",
      "97.80000000000001%: there\n",
      "97.9%: there\n",
      "97.80000000000001%: there\n",
      "98.30000000000001%: there\n",
      "98.0%: there\n",
      "97.4%: there\n",
      "97.30000000000001%: there\n",
      "97.2%: there\n",
      "98.2%: there\n",
      "97.7%: there\n",
      "97.7%: there\n",
      "97.80000000000001%: there\n",
      "97.30000000000001%: there\n",
      "97.9%: there\n",
      "98.2%: there\n",
      "98.0%: there\n",
      "98.0%: there\n",
      "98.2%: there\n",
      "98.10000000000001%: there\n",
      "97.60000000000001%: there\n",
      "97.4%: there\n",
      "98.0%: there\n",
      "97.9%: there\n",
      "98.10000000000001%: there\n",
      "98.0%: there\n"
     ]
    }
   ],
   "source": [
    "for i in range(DICTIONARYSIZE):\n",
    "    a = model.predict(np.array([wordToVector[wordsByPopularity[i]]]))[0]\n",
    "    printConfidence(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printConfidence(prediction):\n",
    "    for i,confidence in enumerate(prediction):\n",
    "        if confidence > 0.01:\n",
    "            print(str((confidence * 100) - (confidence * 100)%0.1) + \"%: \" + wordsByPopularity[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
